{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy as Dcp\n",
    "from math import sqrt, floor, ceil\n",
    "import os\n",
    "import time\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers as optmz\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#np.random.seed(392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------------------\n",
    "## MINST DATA AND PERDICTION\n",
    "## ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "(train_digits, train_labels), (test_digits, test_labels) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "height = train_digits.shape[1]\n",
    "width = train_digits.shape[2]\n",
    "channels = 1\n",
    "\n",
    "train_digits = np.reshape(train_digits, (train_digits.shape[0], height, width, channels))\n",
    "test_digits = np.reshape(test_digits, (test_digits.shape[0], height, width, channels))\n",
    "\n",
    "#normalize values to   0 <= value <= 255\n",
    "train_digits = train_digits.astype('float32') / 255.0\n",
    "test_digits = test_digits.astype('float32') / 255.0\n",
    "\n",
    "total_categories = 10\n",
    "op_label_train = to_categorical(train_labels, num_classes=10)\n",
    "op_label_test = to_categorical(test_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    indexes = np.random.permutation(len(train_digits))\n",
    "    \n",
    "#shuffled the training dataset\n",
    "train_digits = train_digits[indexes]\n",
    "op_label_train = op_label_train[indexes]\n",
    "\n",
    "\n",
    "validity_fract = 0.15\n",
    "fract = int(len(train_digits) * validity_fract)\n",
    "\n",
    "val_digits = train_digits[:fract,:]\n",
    "op_label_val = op_label_train[:fract,:]\n",
    "\n",
    "train_digits = train_digits[fract:,:]\n",
    "op_label_train = op_label_train[fract:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda\\envs\\withKeras\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add( Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',\n",
    "                 input_shape=(height,width,channels) )\n",
    "         )\n",
    "model.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "model.add( Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu') )\n",
    "model.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "model.add( Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu') )\n",
    "model.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add( Dense(128, activation='relu') )\n",
    "model.add( Dense(10,activation='softmax') )\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#FOR BACKPROP\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add( Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',\n",
    "                 input_shape=(height,width,channels) )\n",
    "         )\n",
    "model1.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "model1.add( Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu') )\n",
    "model1.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "model1.add( Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu') )\n",
    "model1.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "model1.add( Dense(128, activation='relu') )\n",
    "model1.add( Dense(10,activation='softmax') )\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 11\n",
    "BATCH_SIZE = 204\n",
    "\n",
    "#results = model1.fit(train_digits, op_label_train, epochs=NUM_EPOCH, batch_size=BATCH_SIZE,\n",
    "#                   validation_data=(val_digits,op_label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY FOR PLOTTING METRICS AFTER RUN\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#his = results.history\n",
    "\n",
    "#plt.gcf().set_size_inches(18.5, 10.5, forward=True)\n",
    "#plt.xlabel('Epochs', fontsize=18)\n",
    "#plt.ylabel('Stuff', fontsize=16)\n",
    "\n",
    "#xaxis = [i for i in range(len(his['loss']))]\n",
    "\n",
    "#plt.plot(xaxis, his['val_loss'])\n",
    "#plt.plot(xaxis, his['val_acc'])\n",
    "#plt.plot(xaxis, his['loss'])\n",
    "#plt.plot(xaxis, his['acc'])\n",
    "\n",
    "#plt.legend(list(his.keys()), loc='upper left')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rough\n",
    "#test_loss, test_acc = model.evaluate(test_digits, op_label_test, batch_size=BATCH_SIZE)\n",
    "#print('TEST Loss:%s  Acc:%s' %(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_dic = {}\n",
    "for i, each_layer in enumerate(model1.layers):\n",
    "    wts = each_layer.get_weights()\n",
    "    if not wts:\n",
    "        wts = [[],[]]\n",
    "    shape_dic[str(i)] = {'wt':np.shape(wts[0]), 'bi':np.shape(wts[1]), 'name':each_layer.name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_model_params(model_obj, meta_info=shape_dic):\n",
    "    '''\n",
    "    meta_info : dictionary that contains info abt each layer\n",
    "            wts and biases dimentions\n",
    "    RETURNS: a python list of \"flattened\" wts. and biases\n",
    "    '''\n",
    "    i = 0\n",
    "    all_li = []\n",
    "    for i, ly in enumerate(model_obj.layers):\n",
    "        wts = ly.get_weights()\n",
    "        if not wts:\n",
    "            continue\n",
    "        wt = wts[0]\n",
    "        bia = wts[1]\n",
    "        _wt = wt.reshape(\n",
    "                            (1, np.prod(meta_info[str(i)]['wt']))\n",
    "                        ).tolist()[0]\n",
    "        _bi = bia.reshape(\n",
    "                            (1, np.prod(meta_info[str(i)]['bi']))\n",
    "                        ).tolist()[0]\n",
    "        all_li += _wt\n",
    "        all_li += _bi\n",
    "    return all_li\n",
    "\n",
    "def unflatten_model_params(wt_bi_list, meta_info=shape_dic):\n",
    "    '''\n",
    "    Takes a long list of weighs and biases obtained from\n",
    "    \"flatten_model_params\" and then retuns a dictionary with\n",
    "    weights and biases as np.arrays per layer\n",
    "    '''\n",
    "    i=0\n",
    "    all_dic = {}\n",
    "    for lyr, dims in meta_info.items():\n",
    "        wt_shape = dims['wt']\n",
    "        bias_shape = dims['bi']\n",
    "        _wt = wt_bi_list[0: np.prod(wt_shape)]\n",
    "        wt_bi_list = wt_bi_list[np.prod(wt_shape) : ]\n",
    "        _bt = wt_bi_list[0: np.prod(bias_shape) ]\n",
    "        wt_bi_list = wt_bi_list[np.prod(bias_shape) : ]\n",
    "        all_dic[lyr] = {\n",
    "                        'wt': np.array(_wt).reshape(wt_shape) , \n",
    "                        'bi': np.array(_bt).reshape(bias_shape),\n",
    "                        'name': dims['name']\n",
    "                        }\n",
    "    return all_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model_obj, X, Y, ret_acc=False,\n",
    "              batches=BATCH_SIZE):\n",
    "    '''\n",
    "    Assumes that wts and biases are set\n",
    "    in \"model_obj\". X is observations\n",
    "    Y is label\n",
    "    \n",
    "    BatchSize is set to \"51\"\n",
    "    \n",
    "    Returns (loss)*-1 if \"ret_acc\"\n",
    "    is not set else\n",
    "    accuracy*100 is also returned\n",
    "    '''\n",
    "    loss, acc = model_obj.evaluate(X, Y, batch_size=batches)\n",
    "    if ret_acc:\n",
    "        return (acc*100, (loss)*-1)\n",
    "    return (loss)*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_func(f, arr, feats=test_digits,# train_digits,\n",
    "                 oplabes=op_label_test, #train,\n",
    "                 model_obj=model,\n",
    "                 reshape_func=unflatten_model_params):\n",
    "    '''\n",
    "    f: a function that needs to be ecaluated\n",
    "    arr: the input to f (hopefully an array of position\n",
    "          in search space)\n",
    "    \n",
    "    returns a num which is the fitness values at \"arr\"\n",
    "    '''\n",
    "    if not f:\n",
    "        raise Exception('Function not set')\n",
    "        \n",
    "    for ly, ars in reshape_func(arr).items():\n",
    "            if ars['wt'].size == 0 and ars['bi'].size == 0:\n",
    "                continue\n",
    "            model_obj.layers[int(ly)].set_weights(\n",
    "                                                [ ars['wt'], ars['bi'] ]\n",
    "                                             )\n",
    "    return f(model_obj, feats, oplabes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------------------\n",
    "## END of MINST DATA AND PERDICTION\n",
    "## ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------------------\n",
    "## CRYOTHERAPY DATA AND PREDECTION\n",
    "## ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_path = os.path.sep.join(['C:','Users','prajw','Downloads','Temp_Folder'])\n",
    "filename = 'Cryotherapy.xlsx'\n",
    "\n",
    "cvs = pd.read_excel( os.path.sep.join([base_path, filename]) )\n",
    "\n",
    "op_label_all = cvs[['Result_of_Treatment']]\n",
    "features_all = cvs[['sex','age','Time','Number_of_Warts','Type','Area']]\n",
    "\n",
    "#train data\n",
    "features = features_all[:int(len(features_all)*0.85)]\n",
    "op_label = op_label_all[:int(len(op_label_all)*0.85)]\n",
    "\n",
    "#test data\n",
    "features_test = features_all[int(len(features_all)*0.85):]\n",
    "op_label_test = op_label_all[int(len(op_label_all)*0.85):]\n",
    "\n",
    "def rms_error(model_obj, x_matrix, y_matrix):\n",
    "    '''\n",
    "    x_matrix and y_matrix assumed to be numpy arrs\n",
    "    '''\n",
    "    N = np.shape(x_matrix)[0]\n",
    "    diff = model_obj.predict(x_matrix) - np.array(y_matrix)\n",
    "    return (-1)*sqrt( np.sum(diff**2)/N )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "dense = Dense\n",
    "\n",
    "model.add( dense(6, activation='relu', input_dim=6, name='input') )\n",
    "model.add( dense(12, activation='relu', name='hidden_1') )\n",
    "model.add( dense(1, activation='sigmoid', name='output') )\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model1 = Sequential()\n",
    "dense = Dense\n",
    "\n",
    "model1.add( dense(6, activation='relu', input_dim=6, name='input') )\n",
    "model1.add( dense(12, activation='relu', name='hidden_1') )\n",
    "model1.add( dense(1, activation='sigmoid', name='output') )\n",
    "\n",
    "model1.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#rough\n",
    "#our_epochs = 1000\n",
    "#for i in range(our_epochs):\n",
    "model1.fit(features, op_label, epochs=200, batch_size=len(features))\n",
    "#    rms_err_backprop = rms_error(model, features, op_label)\n",
    "    #keras_time_series.append(rms_err_backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STORE WEIGHTS AND BIASES META DATA IN \n",
    "# 'shape_dic'. INFO OF ONLY DIMENTIONS ARE KEPT\n",
    "shape_dic = {}\n",
    "i = 0\n",
    "for ly in model1.layers:\n",
    "    wts = ly.get_weights()\n",
    "    wt = wts[0]\n",
    "    bia = wts[1]\n",
    "    shape_dic[str(i)] = {'wt':wt.shape, 'bi':bia.shape, 'name':ly.name}\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def flatten_model_params(model_obj, meta_info=shape_dic):\n",
    "    '''\n",
    "    meta_info : dictionary that contains info abt each layer\n",
    "            wts and biases dimentions\n",
    "    RETURNS: a python list of \"flattened\" wts. and biases\n",
    "    '''\n",
    "    i = 0\n",
    "    all_li = []\n",
    "    for ly in model_obj.layers:\n",
    "        wts = ly.get_weights()\n",
    "        wt = wts[0]\n",
    "        bia = wts[1]\n",
    "        _wt = wt.reshape(\n",
    "                            (1, meta_info[str(i)]['wt'][0]*meta_info[str(i)]['wt'][1])\n",
    "                        ).tolist()[0]\n",
    "        _bi = bia.reshape(\n",
    "                            (1, meta_info[str(i)]['bi'][0])\n",
    "                        ).tolist()[0]\n",
    "        i += 1\n",
    "        all_li += _wt\n",
    "        all_li += _bi\n",
    "    return all_li\n",
    "\n",
    "def unflatten_model_params(wt_bi_list, meta_info=shape_dic):\n",
    "    '''\n",
    "    Takes a long list of weighs and biases obtained from\n",
    "    \"flatten_model_params\" and then retuns a dictionary with\n",
    "    weights and biases as np.arrays per layer\n",
    "    '''\n",
    "    i=0\n",
    "    all_dic = {}\n",
    "    for lyr, dims in meta_info.items():\n",
    "        wt_shape = dims['wt']\n",
    "        bias_shape = dims['bi']\n",
    "        _wt = wt_bi_list[0: wt_shape[0]*wt_shape[1]]\n",
    "        wt_bi_list = wt_bi_list[wt_shape[0]*wt_shape[1] : ]\n",
    "        _bt = wt_bi_list[0: bias_shape[0] ]\n",
    "        wt_bi_list = wt_bi_list[bias_shape[0] : ]\n",
    "        all_dic[lyr] = {\n",
    "                        'wt': np.array(_wt).reshape(wt_shape) , \n",
    "                        'bi': np.array(_bt).reshape(bias_shape)\n",
    "                        }\n",
    "    return all_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def fitness_func(f, arr, feats=features, oplabes=op_label,\n",
    "                 model_obj=model,\n",
    "                 reshape_func=unflatten_model_params):\n",
    "    '''\n",
    "    f: a function that needs to be ecaluated\n",
    "    arr: the input to f (hopefully an array of position\n",
    "          in search space)\n",
    "    \n",
    "    returns a num which is the fitness values at \"arr\"\n",
    "    '''\n",
    "    if not f:\n",
    "        raise Exception('Function not set')\n",
    "        \n",
    "    for ly, ars in reshape_func(arr).items():\n",
    "            model_obj.layers[int(ly)].set_weights(\n",
    "                                                [ ars['wt'], ars['bi'] ]\n",
    "                                             )\n",
    "    return f(model_obj, feats, oplabes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------------------\n",
    "## END OF CRYOTHERAPY DATA AND PREDECTION\n",
    "## ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO PARAMETERS\n",
    "\n",
    "#Population\n",
    "population = []\n",
    "population_size = 2 #30 #No of particles we want to exist\n",
    "\n",
    "#Max allowable strength of a baboon\n",
    "st_max = 100\n",
    "\n",
    "#Min strength of a baboon\n",
    "st_min = 5\n",
    "\n",
    "#Max Stride length\n",
    "stride_max = 200\n",
    "\n",
    "#Min stride lenth\n",
    "stride_min = 3\n",
    "\n",
    "#Fencing archive\n",
    "m = 15 #Number of records of fences we would keep\n",
    "Fence = [] #List of elems : objs of class \"fence_elem\"\n",
    "\n",
    "#Global best position found as yet\n",
    "gbest = None #Position of the best solution so far\n",
    "gbest_value = -99999999999 #fitness value of the current gbest\n",
    "gbest_placeholder_radius = None\n",
    "\n",
    "#Importance to pbest and gbest values\n",
    "pb_wt = 2\n",
    "gb_wt = 6\n",
    "\n",
    "\n",
    "#Dicts to track distance to other fences and baboons\n",
    "\n",
    "#it will consists of keys-(ID1,ID2) with value as\n",
    "#distance. this relation is symmetric as in\n",
    "#(ID1,ID2) == (ID2,ID1)\n",
    "bab2bab = {}\n",
    "\n",
    "#This will be of form: key - (ID, Fence index)\n",
    "#value is the distance of baboon to fence\n",
    "bab2fence = {}\n",
    "\n",
    "#Number of iterations of the entire cycle that we would\n",
    "#run the algo for before stopping\n",
    "num_loops = 1 #3000\n",
    "\n",
    "#Probablity of how sparse the 'mask' vector is when \n",
    "#choosing a new point inside the Fence element. It \n",
    "#fraction of pts we want to move in. Should be b/w\n",
    "#1 and 0. 0 means we remain at the same place\n",
    "p_fract = 0.2\n",
    "\n",
    "#Fraction of population that (stochastically)\n",
    "#must move per iteration. Set to 0 if none should\n",
    "#move, set to 1 if all must move\n",
    "fract_moving_pop = 1 #0.8\n",
    "\n",
    "#Fraction of elite Fence elements that will \n",
    "#exploit the space within them. Set 0 so \n",
    "#that none of the fence elements move, 1\n",
    "#for all of them to move\n",
    "fence_elitism_fract = 0.4\n",
    "\n",
    "#Probablity of an non elite fence element to\n",
    "#move(exploit) its arear within itself\n",
    "fence_non_elite_prob = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_cal(pos1, pos2):\n",
    "    '''\n",
    "    pos1 and pos2 are positions\n",
    "    '''\n",
    "    if len(pos1) != len(pos2):\n",
    "        raise Exception('Length of pos1 and pos2 not equal %s N %s' \\\n",
    "                       %(pos1,pos2))\n",
    "        \n",
    "    return sqrt( ((np.array(pos1) - np.array(pos2))**2).sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_vector(vector, factor=None):\n",
    "    '''\n",
    "    Scales a numpy vector by factor\n",
    "    '''\n",
    "    if factor:\n",
    "        if not np.all(vector==0):\n",
    "            #cannot scale 0 vector\n",
    "            vector = (vector / np.linalg.norm(vector) ) * factor\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector(final, begin, scale=None, ret_list=False):\n",
    "    '''\n",
    "    Given 2 points final and begin in some n Dimention\n",
    "    Return the vector of these 2 points\n",
    "    \n",
    "    ret_list: Flag to indicate of vector returned should be\n",
    "                in a list format\n",
    "                \n",
    "    We assume vectors in numpy nD array form\n",
    "    '''\n",
    "    if len(final) != len(begin):\n",
    "        #Dimentions dont match\n",
    "        raise Exception('Dimentions of final pt. and begin pt. do not match Final:%s Begin:%s'\n",
    "                       %(final,begin))\n",
    "    vec = np.array(final) - np.array(begin)\n",
    "    vec = scale_vector(vec, factor=scale)\n",
    "    if ret_list:\n",
    "        return vec.tolist()\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultant_vector(list_of_vec, ret_list=False):\n",
    "    '''\n",
    "    Returns the resultant of the vectors in \"list_of_vec\"\n",
    "    (default form numpy array)\n",
    "    ret_list: If set returns resultant as a list not numpy arr\n",
    "    '''\n",
    "    if not list_of_vec:\n",
    "        raise Exception('Empty list of vectors to be added')\n",
    "    vecs = list(map(lambda x: np.array(x) , list_of_vec))\n",
    "    result = vecs[0]\n",
    "    for i in range(1,len(vecs)):\n",
    "        result += vecs[i]\n",
    "    if ret_list:\n",
    "        return result.tolist()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_space_violations(own_ID, own_space, janta=population, fences=Fence):\n",
    "                       #to_baboon=bab2bab, to_fence=bab2fence):\n",
    "    '''\n",
    "    Depending on the distance of the particle to other entites\n",
    "    we check if the personal spaces overlap or not\n",
    "    \n",
    "    Returns a list of positions with overlap, the overlap might\n",
    "    be with a fence or a personal space\n",
    "    '''\n",
    "    global bab2bab\n",
    "    global bab2fence\n",
    "    \n",
    "    viol = []\n",
    "    for single in janta:\n",
    "        if single.ID == own_ID:\n",
    "            continue\n",
    "        key = (own_ID, single.ID) if (own_ID, single.ID) in bab2bab \\\n",
    "                else (single.ID, own_ID)\n",
    "        dist = bab2bab[key]\n",
    "        if dist < own_space + single.personal_space[1]: #The radius of hyper-sphere\n",
    "            viol.append(single.curr_position)\n",
    "            #print('%s is in violation of space of ID:%s' %(own_ID,single.ID)) #DEBUG\n",
    "            \n",
    "    for index, picket in enumerate(fences):\n",
    "        dist = bab2fence[(own_ID, index)]\n",
    "        if dist < own_space + picket.radius:\n",
    "            #print('%s is in violation of space of FENCE:%s' %(own_ID,index)) #DEBUG\n",
    "            viol.append(picket.position)\n",
    "            \n",
    "    return viol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_to_everything(own_pos, own_ID, janta=population, fences=Fence,\n",
    "                       #to_baboon=bab2bab, to_fence=bab2fence,\n",
    "                       dist_func=dist_cal):\n",
    "    '''\n",
    "    This function is to be called by a particle at\n",
    "    position \"own_pos\" with ID:own_ID. \"janta\" and\n",
    "    \"fences\" are popluation list and list of fence\n",
    "    archive\n",
    "    \n",
    "    It populates the \"bab2bab\" dict and \"to_fenc\n",
    "    e\" in the format specified when these dicts ar\n",
    "    e first created\n",
    "    '''\n",
    "    global bab2bab\n",
    "    global bab2fence\n",
    "    \n",
    "    for single in janta:\n",
    "        if single.ID == own_ID:\n",
    "            continue\n",
    "        #dist to other baboons\n",
    "        key = (own_ID, single.ID) if (own_ID, single.ID) in bab2bab \\\n",
    "                else ((single.ID, own_ID) if (single.ID, own_ID) in bab2bab \\\n",
    "                                          else (own_ID, single.ID))\n",
    "        bab2bab[key] = dist_func(own_pos, single.curr_position)\n",
    "        \n",
    "    for index, picket in enumerate(fences):\n",
    "        bab2fence[(own_ID,index)] = dist_func(own_pos, picket.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fence_elem:\n",
    "    def __init__(self, fence_center, fence_radius, fitness_value,\n",
    "                fit_sub=fitness_func, f=model_eval):\n",
    "        \n",
    "        global gbest\n",
    "        global gbest_value\n",
    "        \n",
    "        hi = 1\n",
    "        lo = hi*-1\n",
    "        self.position = fence_center #The Center of the Fence\n",
    "        self.radius = fence_radius\n",
    "        self.fit_val = fitness_value\n",
    "        a_pos = [i + (np.random.uniform(lo,hi)) for i in self.position]\n",
    "        a_counter = 0\n",
    "        while dist_cal(a_pos, self.position) > self.radius:\n",
    "            if a_counter < 25:\n",
    "                print('Dist of random Pt outside Fence element') #DEBUG\n",
    "                print('DIst: %s -- Radius:%s' %(dist_cal(a_pos, self.position), self.radius)) #DEBUG\n",
    "                hi /= 2\n",
    "                lo /= 2\n",
    "                a_pos = [i + (np.random.uniform(lo,hi)) for i in self.position]\n",
    "            else:\n",
    "                print('Take same position as FenceCenter') #DEBUG\n",
    "                a_pos = Dcp(self.position) #Just take the same position as the fence center\n",
    "                self.radius *= 1.1 #Increase fence radius so that this can move on\n",
    "                a_counter = 0\n",
    "                hi = 1\n",
    "                lo = hi*-1\n",
    "                continue\n",
    "            a_counter += 1\n",
    "                \n",
    "        #print('DIst: %s -- Radius:%s' %(dist_cal(a_pos, self.position), self.radius)) #DEBUG\n",
    "        it_fit = fit_sub(f,a_pos)\n",
    "        if gbest_value < it_fit:\n",
    "            gbest_value = it_fit\n",
    "            gbest = Dcp(a_pos)\n",
    "        self.settler = self.Settler(a_pos,Dcp(self.radius),\n",
    "                                    Dcp(self.fit_val), Dcp(self.position))\n",
    "        \n",
    "    class Settler:\n",
    "        def __init__(self, pos, limit, fitness_score,\n",
    "                    fence_center):\n",
    "            self.edge_dist = limit\n",
    "            self.position = pos\n",
    "            self.fitness_score = fitness_score\n",
    "            self.fence_center = fence_center\n",
    "            self.avg_score = fitness_score\n",
    "            self.N = 1\n",
    "            \n",
    "        def recalculate_avg_score(self):\n",
    "            '''\n",
    "            I assume that the self.fitness_score\n",
    "            is already calculated and stored\n",
    "            Calling this function should be after\n",
    "            the new fitness score is calculated\n",
    "            '''\n",
    "            \n",
    "            self.avg_score = (self.avg_score*self.N + self.fitness_score)/(self.N + 1)\n",
    "            self.N += 1\n",
    "        \n",
    "        def move_settler(self, fit_fun=fitness_func,f=model_eval,\n",
    "                        grad_step_size=0.25):\n",
    "            \n",
    "            global gbest\n",
    "            global gbest_value\n",
    "            \n",
    "            tick_1 = time.time()  #FOR TIME\n",
    "            \n",
    "            interim_fitness = -9999999\n",
    "            while abs(interim_fitness - self.fitness_score) > 0.1:\n",
    "                mask = np.random.choice([0, 0.25, -0.25], p=[1 - p_fract, p_fract/2, p_fract/2],\n",
    "                                       size=np.size(self.position))\n",
    "                new_pos = list(np.array(self.position) + mask)\n",
    "                interim_fitness = fit_fun(f, new_pos)\n",
    "                #print('InLoop taking %s -- %s' %(interim_fitness, self.fitness_score))  #DELETE THIS\n",
    "            \n",
    "            #for dim in range(len(self.position)):\n",
    "            #    temp_mask[dim] = grad_step_size\n",
    "            #    new_pos = list(np.array(self.position) + temp_mask)\n",
    "            #    if fit_fun(f, new_pos) > self.fitness_score:\n",
    "            #        mask[dim] += grad_step_size\n",
    "                \n",
    "            #    temp_mask[dim] = grad_step_size * -1\n",
    "            #    new_pos = list(np.array(self.position) + temp_mask)\n",
    "            #    if fit_fun(f, new_pos) > self.fitness_score:\n",
    "            #        mask[dim] += (grad_step_size * -1)\n",
    "                \n",
    "            #    temp_mask = np.zeros_like(self.position)\n",
    "                \n",
    "            #    tick_2 = time.time()  #FOR TIME\n",
    "                \n",
    "            tick_3 = time.time()  #FOR TIME\n",
    "            \n",
    "            #new_pos = np.array(self.position) + mask\n",
    "            \n",
    "            tick_4 = time.time()  #FOR TIME\n",
    "            \n",
    "            heading = make_vector(new_pos, self.position,\n",
    "                                  scale=np.random.uniform(0.1, 1) #Adjust this range to adjust stride\n",
    "                                 )\n",
    "            \n",
    "            tick_5 = time.time()  #FOR TIME\n",
    "            \n",
    "            interim_pos = (np.array(self.position) + heading).tolist()\n",
    "            if dist_cal(interim_pos, self.fence_center) > self.edge_dist:\n",
    "                \n",
    "                tick_5_5 = time.time()  #FOR TIME\n",
    "                \n",
    "                self.reset_settler()\n",
    "                \n",
    "                tick_5_6 = time.time()  #FOR TIME\n",
    "                #print('ResetSettler:%s' %(tick_5_6 - tick_5_5)) #DELETE THIS\n",
    "                return\n",
    "                \n",
    "            tick_6 = time.time()  #FOR TIME\n",
    "            \n",
    "            interim_fitness = fit_fun(f, interim_pos)\n",
    "            \n",
    "            tick_7 = time.time()  #FOR TIME\n",
    "            \n",
    "            if interim_fitness > self.fitness_score:\n",
    "                self.fitness_score = interim_fitness\n",
    "                self.recalculate_avg_score()\n",
    "                self.position = Dcp(interim_pos)\n",
    "                if interim_fitness > gbest_value:\n",
    "                    gbest_value = interim_fitness\n",
    "                    gbest = Dcp(interim_pos)\n",
    "                    \n",
    "                #print('TimeBumpUPfitness :%s' %(time.time() - tick_7))  #DELET THIS\n",
    "            else:\n",
    "                self.position = Dcp(new_pos)\n",
    "                self.fitness_score = fit_fun(f, new_pos)\n",
    "                self.recalculate_avg_score()\n",
    "                if self.fitness_score > gbest_value:\n",
    "                    gbest_value = self.fitness_score\n",
    "                    gbest = Dcp(self.position)\n",
    "                \n",
    "                #print('TimeSmplUpdate :%s' %(time.time() - tick_7))  #DELET THIS\n",
    "                \n",
    "            #DELETE THE FOLLOWING, ONLY FOT DEBUG\n",
    "            #print('TimePickGradient:%s -- MakeVector: %s -- DistNreset: %s -- CalFItness: %s' \\\n",
    "            #     %(tick_3 - tick_1, tick_5 - tick_4, tick_6 - tick_5, tick_7 - tick_6))\n",
    "                \n",
    "        def reset_settler(self, fit_fun=fitness_func,f=model_eval):\n",
    "            \n",
    "            global gbest\n",
    "            global gbest_value\n",
    "            \n",
    "            _hi = 1\n",
    "            _lo = _hi * -1\n",
    "            curr = [(i + np.random.uniform(_lo,_hi))  for i in self.fence_center]\n",
    "            a_count = 0\n",
    "            while dist_cal(self.fence_center, curr) > self.edge_dist:\n",
    "                if a_count < 25:\n",
    "                    print('reset_settler has resettled outside fence') #DEBUG\n",
    "                    _hi /= 2\n",
    "                    _lo /= 2\n",
    "                    curr = [(i + np.random.uniform(_lo,_hi))  for i in self.fence_center]\n",
    "                else:\n",
    "                    curr = Dcp(self.fence_center)\n",
    "                a_count += 1\n",
    "                \n",
    "            self.position = Dcp(curr)\n",
    "            self.fitness_score = fit_fun(f,self.position)\n",
    "            self.recalculate_avg_score()\n",
    "            if self.fitness_score > gbest_value:\n",
    "                    gbest_value = self.fitness_score\n",
    "                    gbest = Dcp(self.position)\n",
    "        \n",
    "def update_fences(fence=Fence, capacity=m):\n",
    "    '''\n",
    "    This function will be called after all particles have \n",
    "    moved and the gbest is to be inducted into the Fences\n",
    "    '''\n",
    "    global gbest\n",
    "    global gbest_placeholder_radius\n",
    "    global gbest_value\n",
    "    \n",
    "    if not gbest or not gbest_placeholder_radius or not gbest_value:\n",
    "        return\n",
    "    \n",
    "    g_B = fence_elem(gbest, gbest_placeholder_radius, gbest_value)\n",
    "    \n",
    "    if len(fence) < capacity:\n",
    "        status = any( [\n",
    "                        bool(dist_cal(gbest, i.position) < 1) for i in fence\n",
    "                        ] )\n",
    "        if not status:\n",
    "            fence.append(g_B)\n",
    "    else:\n",
    "        near = 99999999\n",
    "        idx = None\n",
    "        for index, elem in enumerate(fence):\n",
    "            dst = dist_cal(gbest + [gbest_placeholder_radius] , \n",
    "                            elem.position + [elem.radius])\n",
    "            if dst < near:\n",
    "                near = dst\n",
    "                idx = index\n",
    "        if not idx:\n",
    "            if fence[idx].fit_val < gbest_value:\n",
    "                fence[idx] = g_B\n",
    "        #else no changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baboon:\n",
    "    def __init__(self, ID, curr_position, fitness_function=fitness_func,\n",
    "                pt2vec=make_vector, vec_scale=scale_vector,\n",
    "                actual_func_fit=model_eval,fit_val_negative=True):\n",
    "        '''\n",
    "        Note: The correct order is to set \"strength\" before setting\n",
    "                \"personal_space\" through the class methods\n",
    "        '''\n",
    "        #The following are taken as global in this class, if a custom\n",
    "        #object is to be used, create own defination of the object\n",
    "        # - st_min and st_max\n",
    "        # - stride_max and stride_min\n",
    "        # - dist_to_everything and find_space_violations\n",
    "        \n",
    "        self.ID = ID\n",
    "        self.curr_position = curr_position\n",
    "        self.fitness_function = fitness_function\n",
    "        self.actual_func_fit = actual_func_fit\n",
    "        self.fitness_value = fitness_function(actual_func_fit, curr_position)\n",
    "        self.fitness_value_prev = Dcp(self.fitness_value) #For initialization use same\n",
    "        self.strength = np.random.uniform(st_min, st_max)\n",
    "        self.stride = np.random.uniform(stride_min, stride_max)\n",
    "        self.fitness_val_negative = fit_val_negative\n",
    "        #Flag to indicate if fitness values are negative. If so\n",
    "        #then 0.0 is the max value\n",
    "        \n",
    "        self.personal_space = ( self.curr_position ,\n",
    "                               np.random.uniform(st_min, st_max) )\n",
    "        \n",
    "        self.pbest = Dcp(curr_position)\n",
    "        self.pbest_fitness = Dcp(self.fitness_value)\n",
    "        \n",
    "        #function to make a vector and scale vector\n",
    "        self.vec_scale = vec_scale\n",
    "        self.pt2vec = pt2vec\n",
    "        \n",
    "        #Randomly Initialize velocity vector\n",
    "        #it is a numpy array unlike curr_position\n",
    "        #which is going to be a list\n",
    "        self.velocity = self.pt2vec([np.random.normal(abs(i), abs(i)*3) for i in self.curr_position],\n",
    "                                    self.curr_position,\n",
    "                                   scale=self.stride)\n",
    "        \n",
    "        #All of this is only for initializatons \n",
    "        \n",
    "    def set_stridelength(self, fence=Fence):\n",
    "        #StrideLength(t) : StrideLength(t-1) x Fencing x PersonalSpace(t-1) x σ(0,1) -> R\n",
    "        if self.fitness_val_negative:\n",
    "            change = (abs(self.fitness_value_prev) - abs(self.fitness_value))/abs(self.fitness_value_prev)\n",
    "        else:\n",
    "            change = (abs(self.fitness_value_prev) - abs(self.fitness_value))/abs(self.fitness_value_prev)\n",
    "\n",
    "        self.stride = self.stride*(1 + change)\n",
    "        if self.stride > stride_max:\n",
    "            self.stride = stride_max\n",
    "        if self.stride < stride_min:\n",
    "            self.stride = stride_min\n",
    "        \n",
    "    def set_personalspace(self):\n",
    "        #TODO:\n",
    "        #PersonalSpace(t) : Strength(t) x Position(t) -> ( Position(t) , R )\n",
    "        \n",
    "        pos_t , R_t_1 = self.personal_space\n",
    "        change = (self.strength - self.strength_t_1)/self.strength_t_1\n",
    "        R_t_1 *= 1+change        \n",
    "        self.personal_space = (pos_t , R_t_1)\n",
    "        \n",
    "    def set_strength(self):\n",
    "        #Strength(t) : Strength(t – 1) x Change in fitness -> R\n",
    "        if self.fitness_val_negative:\n",
    "            change = (abs(self.fitness_value_prev) - abs(self.fitness_value))/abs(self.fitness_value_prev)\n",
    "        else:\n",
    "            change = (abs(self.fitness_value_prev) - abs(self.fitness_value))/abs(self.fitness_value_prev)\n",
    "            \n",
    "        self.strength_t_1 = Dcp(self.strength)\n",
    "        \n",
    "        self.strength *= (1 + change)\n",
    "        \n",
    "        if self.strength > st_max:\n",
    "            self.strength = st_max\n",
    "        \n",
    "        if self.strength < st_min:\n",
    "            self.strength = st_min\n",
    "    \n",
    "    def move(self, p_weight=pb_wt, g_weight=gb_wt,\n",
    "             vec_adder=resultant_vector, fence=Fence, fence_capacity=m):\n",
    "            #, g_position=gbest,g_value=gbest_value, g_placeholder=gbest_placeholder_radius\n",
    "        '''\n",
    "        '''\n",
    "        #TODO: \n",
    "        #    - get resutant vec and scale it\n",
    "        #    - check if position violates any personal space\n",
    "        #    - if yes move to \"other\" location(s) till no violation of spaces\n",
    "        #    - update curr position\n",
    "        #    - update fitness strength and personal spaces\n",
    "        #    - update gbest with better sol if any\n",
    "        \n",
    "        global gbest_value\n",
    "        global gbest_placeholder_radius\n",
    "        global gbest\n",
    "        global dist_to_everything\n",
    "        global find_space_violations\n",
    "        global bab2bab\n",
    "        global bab2fence\n",
    "        \n",
    "        p_vec = self.pt2vec(self.pbest, self.curr_position, scale=p_weight)\n",
    "        g_vec = self.pt2vec(gbest, self.curr_position, scale=g_weight)\n",
    "        self.velocity = self.vec_scale( vec_adder([p_vec, g_vec, self.velocity]),\n",
    "                            factor=self.stride+(0 if np.random.uniform(0,1) > 0.05 else 100))\n",
    "        \n",
    "        #cal the position, if there is a violation this will not be used. Otherwise\n",
    "        #this will be the final position\n",
    "        interim_position = (np.array(self.curr_position) + self.velocity).tolist()\n",
    "        dist_to_everything(interim_position, self.ID)\n",
    "        all_violations = find_space_violations(self.ID, self.personal_space[1])\n",
    "        while all_violations:\n",
    "            #print('%s ID had violation' %self.ID)\n",
    "            #Centroid of all points in violation\n",
    "            final_pt = np.array(all_violations).sum(axis=0).tolist()\n",
    "            final_vec = self.pt2vec(final_pt, interim_position,\n",
    "                                    scale=(-1)*(self.stride+(0 if np.random.uniform(0,1) > 0.05 else 100)))\n",
    "            interim_position = (final_vec + np.array(interim_position)).tolist()\n",
    "            dist_to_everything(interim_position, self.ID)\n",
    "            all_violations = find_space_violations(self.ID, self.personal_space[1])\n",
    "        \n",
    "        #print('%s ID moved %s' %(self.ID, dist_cal(self.curr_position, interim_position))) #DELETE THIS LATER\n",
    "        self.curr_position = Dcp(interim_position)\n",
    "        \n",
    "        #update all other params like pbest n stuff\n",
    "        self.fitness_value_prev = Dcp(self.fitness_value)\n",
    "        self.fitness_value = self.fitness_function(self.actual_func_fit,\n",
    "                                                   self.curr_position)\n",
    "        self.set_stridelength()\n",
    "        self.set_strength()\n",
    "        self.set_personalspace()\n",
    "        \n",
    "        if self.fitness_value > gbest_value:\n",
    "            gbest_value = Dcp(self.fitness_value)\n",
    "            gbest = Dcp(self.curr_position)\n",
    "            gbest_placeholder_radius = Dcp(self.personal_space[1])\n",
    "            \n",
    "        if self.pbest_fitness < self.fitness_value:\n",
    "            self.pbest_fitness = Dcp(self.fitness_value)\n",
    "            self.pbest = Dcp(self.curr_position)\n",
    "            gbest_placeholder_radius = Dcp(self.personal_space[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_fitness(pop_list):\n",
    "    '''\n",
    "    Given a list of baboon() objs\n",
    "    returns the average fitness\n",
    "    '''\n",
    "    return np.average([i.fitness_value for i in pop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_dist():\n",
    "    '''\n",
    "    Assumes dict \"bab2bab\" is completely\n",
    "    populated and data is upto date\n",
    "    \n",
    "    Onlu calculates avg dist b/w each \n",
    "    baboons\n",
    "    '''\n",
    "    global bab2bab\n",
    "    \n",
    "    if not bab2bab:\n",
    "        raise Exception('bab2bab dict is empty')\n",
    "    \n",
    "    return np.average([dis for key,dis in bab2bab.items()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 527us/step\n",
      "10000/10000 [==============================] - 5s 504us/step\n"
     ]
    }
   ],
   "source": [
    "#INITIALIZE POPULATION\n",
    "for indx, ii in enumerate(np.random.uniform(-1, 1, size=(population_size, len(flatten_model_params(model))))):\n",
    "    #-1000, 1000, size=(population_size, len(flatten_model_params(model))))):\n",
    "    population.append( baboon('bab_'+str(indx), ii.tolist()) )\n",
    "    \n",
    "mx = -999999999999\n",
    "for each in population:\n",
    "    if each.fitness_value > mx:\n",
    "        mx = each.fitness_value\n",
    "        gbest_placeholder_radius = each.personal_space[1]\n",
    "        gbest = Dcp(each.curr_position)\n",
    "gbest_value = mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.57\n",
      "-14.43\n"
     ]
    }
   ],
   "source": [
    "#rouch\n",
    "print('%.2f' %population[0].fitness_value)\n",
    "print('%.2f' %population[0].fitness_value_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running The Main algo...\n",
      "10000/10000 [==============================] - 5s 521us/step\n",
      "Dist of random Pt outside Fence element\n",
      "DIst: 208.76763717759223 -- Radius:90.20910367655628\n",
      "Dist of random Pt outside Fence element\n",
      "DIst: 104.45992493500634 -- Radius:90.20910367655628\n",
      "10000/10000 [==============================] - 5s 506us/step\n",
      "10000/10000 [==============================] - 5s 497us/step\n",
      "Dist of random Pt outside Fence element\n",
      "DIst: 208.86639725223918 -- Radius:67.81622027149385\n",
      "Dist of random Pt outside Fence element\n",
      "DIst: 104.59610358724034 -- Radius:67.81622027149385\n",
      "10000/10000 [==============================] - 6s 551us/step\n"
     ]
    }
   ],
   "source": [
    "#DELETE AFTER TEST RUN\n",
    "gbest_X = []\n",
    "avg_fit_X = []\n",
    "avg_dist = []\n",
    "test_data_acc = []\n",
    "evals = 0\n",
    "\n",
    "print('Running The Main algo...')\n",
    "for loopz in range(num_loops):\n",
    "    for each_bab in population:\n",
    "        each_bab.move()\n",
    "        update_fences()\n",
    "        gbest_X.append(gbest_value)\n",
    "        avg_fit_X.append(get_avg_fitness(population))\n",
    "        evals += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-14.6280', '-14.5176']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rough\n",
    "['%.4f' %i for i in avg_fit_X]\n",
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The running part of the algo\n",
    "\n",
    "#The following lists are for plotting\n",
    "gbest_X = []\n",
    "avg_fit_X = []\n",
    "avg_dist = []\n",
    "test_data_acc = []\n",
    "evals = 0\n",
    "\n",
    "\n",
    "X = test_digits\n",
    "Y = op_label_test\n",
    "\n",
    "\n",
    "print('Running The Main algo...')\n",
    "bigbang = time.time()\n",
    "for loopz in range(num_loops):\n",
    "    start = time.time()\n",
    "    for each_bab in population:\n",
    "        if np.random.uniform(0,1) > fract_moving_pop:\n",
    "            continue\n",
    "        #start_bab_loop = time.time() #FOR TIME\n",
    "        each_bab.move()\n",
    "        #bab_mv_time = time.time() - start_bab_loop #FOR TIME\n",
    "        #start_up_fence = time.time() #FOR TIME\n",
    "        update_fences()\n",
    "        #up_fence_time = time.time() - start_up_fence #FOR TIME\n",
    "        \n",
    "        gbest_X.append(gbest_value)\n",
    "        avg_fit_X.append(get_avg_fitness(population))\n",
    "        evals += 1\n",
    "        #bab_loop = time.time() - start_bab_loop #FOR TIME\n",
    "        #print('\\n') #DELETE THIS\n",
    "        #print('BaboonMove:%s -- FenceUpd:%s -- TotalBaboon:%s' \\\n",
    "        #     %(bab_mv_time, up_fence_time, bab_loop))\n",
    "        \n",
    "    end_bab_loop_all = time.time() #FOR TIME\n",
    "        \n",
    "    if m == len(Fence):\n",
    "        Fence.sort(key=lambda x:x.settler.avg_score, reverse=True)\n",
    "        limt = int(fence_elitism_fract*len(Fence))\n",
    "        for i in range(len(Fence)):\n",
    "            if i <= limt:\n",
    "                Fence[i].settler.move_settler()\n",
    "                Fence[i].settler.move_settler()\n",
    "            elif np.random.uniform(0,1) <= fence_non_elite_prob:\n",
    "                Fence[i].settler.move_settler()\n",
    "                Fence[i].settler.move_settler()\n",
    "    else:\n",
    "        for each_fence in Fence:\n",
    "            #For each baboon iteration move the\n",
    "            #settlers in the fence twice\n",
    "            #start_mv_settler = time.time() #FOR TIME\n",
    "            each_fence.settler.move_settler()\n",
    "            #mv_settler1 = time.time() - start_mv_settler #FOR TIME\n",
    "            each_fence.settler.move_settler()\n",
    "            #mv_settler2 = time.time() + mv_settler1 - start_mv_settler #FOR TIME\n",
    "            #print('Move1:%s -- Move2:%s' %(mv_settler1, mv_settler2))\n",
    "    #print('End of Loop: %s -- Took:%s sec' %(loopz, time.time() - start)) #DEBUG  \n",
    "    #print('EntireFenceLoop:%s\\n\\n' %(time.time() - end_bab_loop_all))\n",
    "    #print('GB:%s -- %s' %(gbest_value, [i.settler.fitness_score for i in Fence]))\n",
    "    \n",
    "    lozz, accr = model.evaluate(X, Y, batch_size=BATCH_SIZE)\n",
    "    test_data_acc.append(accr*100)\n",
    "    \n",
    "    avg_dist.append(get_avg_dist())\n",
    "    \n",
    "print('...Finished Main algo block in %s sec' %(time.time() - bigbang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------\n",
    "## ----------------------------------------------------------------------------------------------\n",
    "### ROUGH AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROGHHH\n",
    "bab_loss = []\n",
    "\n",
    "for i in range(0,1):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add( Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',\n",
    "                     input_shape=(height,width,channels) )\n",
    "             )\n",
    "    model.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "    model.add( Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu') )\n",
    "    model.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "    model.add( Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu') )\n",
    "    model.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add( Dense(128, activation='relu') )\n",
    "    model.add( Dense(10,activation='softmax') )\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    print('done')\n",
    "    for indx, ii in enumerate(np.random.uniform(-1000, 1000, size=(population_size, len(flatten_model_params(model))))):\n",
    "        population.append( baboon('bab_'+str(indx), ii.tolist()) )\n",
    "    \n",
    "    mx = -999999999999\n",
    "    for each in population:\n",
    "        if each.fitness_value > mx:\n",
    "            mx = each.fitness_value\n",
    "            gbest_placeholder_radius = each.personal_space[1]\n",
    "            gbest = Dcp(each.curr_position)\n",
    "    gbest_value = mx\n",
    "    \n",
    "    a_sum = 0\n",
    "    for each_bab in population:\n",
    "        each_bab.move()\n",
    "        a_sum += each_bab.fitness_value\n",
    "        \n",
    "    bab_loss.append( a_sum/population_size )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last 2 is accuracy test data EA , backprop\n",
    "import pickle\n",
    "with open('gbest_X__avg_fit_X__avg_dist__test_data_acc__mseTrainBackProp__POP80_TRIAL2.pkl', 'wb') as ff:\n",
    "    pickle.dump([gbest_X, avg_fit_X, avg_dist, test_data_acc, 0.1734, 78.57142857142857, 71.42857142857143],ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ROUGHHHH\n",
    "\n",
    "back_loss = []\n",
    "\n",
    "NUM_EPOCH = 1\n",
    "BATCH_SIZE = 204\n",
    "X = test_digits\n",
    "Y = op_label_test\n",
    "\n",
    "for i in range(0,1):\n",
    "    model1 = Sequential()\n",
    "\n",
    "    model1.add( Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',\n",
    "                     input_shape=(height,width,channels) )\n",
    "             )\n",
    "    model1.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "    model1.add( Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu') )\n",
    "    model1.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "    model1.add( Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu') )\n",
    "    model1.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "    model1.add(Flatten())\n",
    "\n",
    "    model1.add( Dense(128, activation='relu') )\n",
    "    model1.add( Dense(10,activation='softmax') )\n",
    "\n",
    "    model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    #res = model1.fit(X, Y, epochs=NUM_EPOCH, batch_size=BATCH_SIZE, validation_data=(val_digits,op_label_val))\n",
    "    #back_loss.append(res.history['loss'][0])\n",
    "    \n",
    "    #model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model1.fit(X, Y, epochs=NUM_EPOCH, batch_size=BATCH_SIZE, validation_data=(val_digits,op_label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'%.2f' %res.history['loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evals\n",
    "model1.layers[2].get_weights()[0]\n",
    "#unflatten_model_params(flatten_model_params(model1, shape_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read trial files and get accuracy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mymod = []\n",
    "back = []\n",
    "mseback = []\n",
    "gbest_max = []\n",
    "for i in range(2,12):\n",
    "    if i == 6:\n",
    "        continue\n",
    "    filename='gbest_X__avg_fit_X__avg_dist__test_data_acc__mseTrainBackProp__TRIAL'+str(i)+'.pkl'\n",
    "    with open(filename, 'rb') as ff:\n",
    "        AA = pickle.load(ff)\n",
    "    mymod.append(AA[5])\n",
    "    back.append(AA[6])\n",
    "    mseback.append(AA[4])\n",
    "    gbest_max.append(max(AA[0]))\n",
    "    ff.close()\n",
    "        \n",
    "mseback = [i*100 for i in mseback]\n",
    "gbest_max = [100 + i*100 for i in gbest_max]\n",
    "\n",
    "plt.gcf().set_size_inches(18.5, 10.5, forward=True)\n",
    "plt.xlabel('Trials', fontsize=18)\n",
    "plt.ylabel('Percent Acc / MSE*100', fontsize=16)\n",
    "\n",
    "xaxis = [i for i in range(len(mymod))]\n",
    "\n",
    "plt.plot(xaxis, mymod )\n",
    "plt.plot(xaxis, back )\n",
    "plt.plot(xaxis, mseback )\n",
    "plt.plot(xaxis, gbest_max )\n",
    "\n",
    "plt.legend(['MyAcc','BackPropAcc', 'MSE*100 Back', 'Gbest/Trial *-100'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find sq error\n",
    "sm = 0\n",
    "cnt = 0\n",
    "for j,ii in enumerate(model.predict(features_test)):\n",
    "    #if ii[0] < 0.5:\n",
    "    #    k = 0\n",
    "    #else:\n",
    "    #    k = 1\n",
    "    k = ii[0]\n",
    "    sm += abs(k - op_label_test.iloc[j][0])\n",
    "    print('%s -- %s' %(k , op_label_test.iloc[j][0]))\n",
    "    cnt += 1\n",
    "print(sqrt(sm/cnt))\n",
    "#print(ii[0] , op_label.iloc[j][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Accr in test data for cryotherapy\n",
    "\n",
    "X = features_test\n",
    "Y = op_label_test\n",
    "\n",
    "sm = 0\n",
    "cnt = 0\n",
    "for j,ii in enumerate(model.predict(X)):\n",
    "    if ii[0] < 0.5:\n",
    "        k = 0\n",
    "    else:\n",
    "        k = 1\n",
    "    #k = ii[0]\n",
    "    sm += (1 if abs(k - Y.iloc[j][0]) == 0 else 0)\n",
    "    print('%s -- %s' %(k , Y.iloc[j][0]))\n",
    "    cnt += 1\n",
    "print((sm/cnt)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Acc in test data for MNIST\n",
    "\n",
    "X = test_digits\n",
    "Y = op_label_test\n",
    "\n",
    "l, a = model.evaluate(X, Y, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbest\n",
    "#flatten_model_params(model)\n",
    "#fitness_func(model_eval, gbest)\n",
    "#gbest_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last 2 is accuracy test data EA , backprop\n",
    "import pickle\n",
    "with open('gbest_X__avg_fit_X__avg_dist__test_data_acc__mseTrainBackProp__POP80_TRIAL2.pkl', 'wb') as ff:\n",
    "    pickle.dump([gbest_X, avg_fit_X, avg_dist, test_data_acc, 0.1734, 78.57142857142857, 71.42857142857143],ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END ROUGH AREA\n",
    "## ----------------------------------------------------------------------------------------------\n",
    "## ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##plt.gca().set_color_cycle(['blue','green'])\n",
    "plt.gcf().set_size_inches(18.5, 10.5, forward=True)\n",
    "plt.xlabel('No of fitness evals', fontsize=18)\n",
    "plt.ylabel('FItness', fontsize=16)\n",
    "\n",
    "xaxis = [i for i in range(len(gbest_X))]\n",
    "\n",
    "#plt.plot(xaxis, gbest_X )\n",
    "plt.plot(xaxis, avg_fit_X)\n",
    "\n",
    "plt.legend(['Gbest','Avg. Fitness'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#del plt\n",
    "\n",
    "plt.gcf().set_size_inches(18.5, 10.5, forward=True)\n",
    "plt.xlabel('No Epochs', fontsize=18)\n",
    "plt.ylabel('Avg. Distance', fontsize=16)\n",
    "xaxis_avgdist = [i for i in range(len(avg_dist))]\n",
    "plt.plot(xaxis_avgdist, avg_dist)\n",
    "plt.legend(['Avg. Dist'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.gcf().set_size_inches(19, 11, forward=True)\n",
    "plt.xlabel('No Epochs', fontsize=18)\n",
    "plt.ylabel('Test Data Acc', fontsize=16)\n",
    "xaxis_avgdist = [i for i in range(len(test_data_acc))]\n",
    "plt.plot(xaxis_avgdist, test_data_acc)\n",
    "plt.legend(['Test Data Acc'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cfm = confusion_matrix(np.argmax(op_label_test,axis=1), np.argmax(model.predict(test_digits),axis=1))\n",
    "print(cfm)\n",
    "print(100*(sum([cfm[i,i] for i in range(cfm.shape[0])])/np.sum(cfm)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
